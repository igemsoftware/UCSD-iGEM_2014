{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### SBiDer\n",
    "\n",
    "\n",
    "import sbider_helper as helper\n",
    "import sqlite3\n",
    "import node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# database_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make SQL command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_sql_insert_command(table_name, table_header_list, insert_data_list):\n",
    "    \"\"\"\n",
    "    Make SQL insert command.\n",
    "    \"\"\"\n",
    "    \n",
    "    sql_insert_into = 'INSERT INTO ' + table_name\n",
    "    \n",
    "    headers = '(' + ','.join(table_header_list) + ')'\n",
    "\n",
    "    insert_data_list_formatted = list(insert_data_list)\n",
    "    for index in range(len(insert_data_list_formatted)):\n",
    "        if isinstance(insert_data_list_formatted[index], str):\n",
    "            insert_data_list_formatted[index] = \"'\" + insert_data_list_formatted[index] + \"'\"\n",
    "        else:\n",
    "            insert_data_list_formatted[index] = str(insert_data_list_formatted[index])\n",
    "    values = 'Values (' + ','.join(insert_data_list_formatted) + ')'\n",
    "\n",
    "    return sql_insert_into + '\\n\\t' + headers + '\\n\\t' + values + ';'\n",
    "\n",
    "\n",
    "def make_sql_select_command(table_name, table_header_list, where_columns=None, where_options=None,\n",
    "                            where_values=None, where_bools=None, group=None, having_columns=None, having_bools=None,\n",
    "                            having_values=None):\n",
    "    \"\"\"\n",
    "    Make SQL select command.\n",
    "    \"\"\"\n",
    "\n",
    "    if where_columns is not None and where_options is not None and where_values is not None and where_bools is not None:\n",
    "        if (len(where_columns) != len(where_options) and len(where_options) != len(where_values) and len(\n",
    "                where_values) != (len(where_bools) - 1)):\n",
    "            raise Exception(\"Invalid argument\")\n",
    "    elif where_columns is not None or where_options is not None or where_values is not None or where_bools is not None:\n",
    "        raise Exception(\"Invalid argument\")\n",
    "\n",
    "    # must have a table name\n",
    "    if table_name is None or len(table_name) == 0:\n",
    "        raise Exception(\"a table name must be provided.\")\n",
    "\n",
    "    sql_select_command = \"SELECT \"\n",
    "    if table_header_list == \"*\":\n",
    "        sql_select_command += \" * \"\n",
    "    else:\n",
    "        for table_header_index in range(len(table_header_list)):\n",
    "            sql_select_command += table_header_list[table_header_index]\n",
    "            if table_header_index != len(table_header_list) - 1:\n",
    "                sql_select_command += \", \"\n",
    "            else:\n",
    "                sql_select_command += \" \"\n",
    "    sql_select_command += \"\\n\" + \"FROM \" + table_name + \" \"\n",
    "\n",
    "    if where_columns is not None:\n",
    "        sql_select_command += \"\\n\" + \"WHERE \"\n",
    "        for where_index in range(len(where_columns)):\n",
    "            sql_select_command += where_columns[where_index] + \" \" + where_options[where_index] + \" \" + str(\n",
    "                where_values[where_index]) + \" \"\n",
    "            if where_index < len(where_bools):\n",
    "                sql_select_command += where_bools[where_index] + \" \"\n",
    "\n",
    "    if group is not None:\n",
    "        sql_select_command += \"\\n\" + \"GROUP BY \" + group\n",
    "\n",
    "    if having_columns is not None and having_bools is not None and having_values is not None:\n",
    "        sql_select_command += \"\\n\" + \"HAVING \" + having_columns + \" \" + having_bools + \" \" + str(having_values)\n",
    "    sql_select_command += \";\"\n",
    "\n",
    "    return sql_select_command\n",
    "\n",
    "\n",
    "def make_sql_update_command(table_name, table_header_list, update_data_list, where_column=\"\",\n",
    "                            where_option=\"\", where_value=\"\"):\n",
    "    \"\"\"\n",
    "    Makes SQL update command.\n",
    "    \"\"\"\n",
    "\n",
    "    sql_update = 'UPDATE ' + table_name\n",
    "\n",
    "    update_values_list = []\n",
    "    for column_name, update_value in zip(table_header_list, update_data_list):\n",
    "        update_values_list.append(str(column_name) + ' = ' + str(update_value))\n",
    "\n",
    "    sql_update_values = 'SET ' + ', '.join(update_values_list)\n",
    "\n",
    "    sql_where = \"\"\n",
    "    if where_column != \"\":\n",
    "        sql_where = \"\\n\" + \" \".join(['WHERE', where_column, where_option, where_value])\n",
    "\n",
    "    '''for where_column_name, where_value , where_op, index in \n",
    "            zip(where_columns, where_values, where_options, range(len(where_columns) + 1)):\n",
    "            if i < len(where_columns) - 1:\n",
    "                hold = sql_where + ' '.join([where_column_name,where_option,str(where_value)]) + ' ' + w_conts[index]\n",
    "            else:\n",
    "                hold = sql_where + ' '.join([where_column_name,where_option,str(where_value)]) + ' ' \n",
    "            sql_where = hold \n",
    "        return update_command + '\\n\\t' + set_str + '\\n\\t' + where_str + ';'''\n",
    "\n",
    "    sql_update_command = sql_update + \"\\n\" + sql_update_values + sql_where + \";\"\n",
    "\n",
    "    return sql_update_command\n",
    "\n",
    "\n",
    "def make_sql_delete_command(table_name):\n",
    "    sql_delete_command = \"DELETE FROM %s;\" % table_name\n",
    "    return sql_delete_command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def db_open(database_file):\n",
    "    \"\"\"\n",
    "    Connect to a database, or create a database if it does not exist.\n",
    "    \"\"\"\n",
    "\n",
    "    connection = sqlite3.connect(database_file)\n",
    "    connection.text_factory = str\n",
    "    cursor = connection.cursor()\n",
    "    return connection, cursor\n",
    "\n",
    "\n",
    "def db_close(connection, cursor):\n",
    "    \"\"\"\n",
    "    Close a database.\n",
    "    \"\"\"\n",
    "\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "\n",
    "\n",
    "def db_create_table(cursor):\n",
    "    \"\"\"\n",
    "    Make database tables.\n",
    "    \"\"\"\n",
    "\n",
    "    #db_drop_all_table(cursor)\n",
    "\n",
    "    ##SQL commands\n",
    "    \n",
    "    # species id - species name - species type\n",
    "    species = '''CREATE TABLE Species (spe_id VARCHAR(50), \n",
    "                                       name VARCHAR(50), \n",
    "                                       type VARCHAR(50));'''\n",
    "    \n",
    "    # plasmid id - plasmid name - pubmed id\n",
    "    plasmid = '''CREATE TABLE Plasmid (pla_id VARCHAR(50), \n",
    "                                       name VARCHAR(50), \n",
    "                                       PMID VARCHAR(50));'''\n",
    "    # operon id - operon name - image path\n",
    "    operon = '''CREATE TABLE Operon (ope_id VARCHAR(50), \n",
    "                                     name VARCHAR(50),\n",
    "                                     image VARCHAR(50));'''\n",
    "\n",
    "    # operon id - plasmid id - direction\n",
    "    po = '''CREATE TABLE PlasmidOperon (ope_id VARCHAR(50), \n",
    "                                        pla_id VARCHAR(50),\n",
    "                                        direction VARCHAR(50));'''\n",
    "\n",
    "    # input transition id - operon id\n",
    "    oit = '''CREATE TABLE OperonInputTransition (it_id VARCHAR(50), \n",
    "                                                 ope_id VARCHAR(50));'''\n",
    "\n",
    "    # input transition id - logic\n",
    "    it = '''CREATE TABLE InputTransition (it_id VARCHAR(50), \n",
    "                                          logic VARCHAR(50));'''\n",
    "\n",
    "    # input id - input transition id - species id - reverse\n",
    "    in_ = '''CREATE TABLE InputTransitionSpecies (in_id VARCHAR(50), \n",
    "                                                  it_id VARCHAR(50), \n",
    "                                                  spe_id VARCHAR(50),\n",
    "                                                  reverse BOOL);'''\n",
    "\n",
    "    # output transition id - operon id\n",
    "    oot = '''CREATE TABLE OperonOutputTransition (ot_id VARCHAR(50),\n",
    "                                                  ope_id VARCHAR(50));'''\n",
    "\n",
    "    # output transition id - logic\n",
    "    ot = '''CREATE TABLE OutputTransition (ot_id VARCHAR(50), \n",
    "                                           logic VARCHAR(50))'''\n",
    "\n",
    "    # output id - output id - species id\n",
    "    out = '''CREATE TABLE OutputTransitionSpecies (out_id VARCHAR(50), \n",
    "                                                   ot_id VARCHAR(50),\n",
    "                                                   spe_id VARCHAR(50));'''\n",
    "\n",
    "    # user id - first name - last name - email - password\n",
    "    login = '''CREATE TABLE User (user_id VARCHAR(50), \n",
    "                                  first_name VARCHAR(50),\n",
    "                                  last_name VARCHAR(50),\n",
    "                                  email VARCHAR(50),\n",
    "                                  password VARCHAR(50));'''\n",
    "\n",
    "    sql_make_table_commands = [species,\n",
    "                               plasmid,\n",
    "                               operon,\n",
    "                               po,\n",
    "                               oit,\n",
    "                               it,\n",
    "                               in_,\n",
    "                               oot,\n",
    "                               ot,\n",
    "                               out,\n",
    "                               login]\n",
    "\n",
    "    for sql_make_table_command in sql_make_table_commands:\n",
    "        cursor.execute(sql_make_table_command)\n",
    "    return cursor\n",
    "\n",
    "\n",
    "def db_drop_table(cursor, table_name):\n",
    "    \"\"\"\n",
    "    Drop a database table.\n",
    "    \"\"\"\n",
    "        \n",
    "    cursor.execute(\"DROP TABLE %s;\" % table_name)\n",
    "    return cursor\n",
    "\n",
    "\n",
    "def db_drop_all_table(cursor):\n",
    "    \"\"\"\n",
    "    Drop all database tables.\n",
    "    \"\"\"\n",
    "\n",
    "    table_names = [\"Species\",\n",
    "                   \"Plasmid\",\n",
    "                   \"Operon\",\n",
    "                   \"PlasmidOperon\",\n",
    "                   \"OperonInputTransition\",\n",
    "                   \"InputTransition\",\n",
    "                   \"InputTransitionSpecies\",\n",
    "                   \"OperonOutputTransition\",\n",
    "                   \"OutputTransition\",\n",
    "                   \"OutputTransitionSpecies\",\n",
    "                   \"User\"]\n",
    "    for table_name in table_names:\n",
    "        cursor.execute(\"DROP TABLE %s;\" % table_name)\n",
    "    return cursor\n",
    "\n",
    "\n",
    "def get_last_row_id(cursor, table_name):\n",
    "    \"\"\"Get the last inserted rowid.\"\"\"\n",
    "    last_id = cursor.execute(\"SELECT rowid FROM %s\" % table_name).fetchall()\n",
    "    last_id = len(last_id)\n",
    "    return last_id\n",
    "\n",
    "\n",
    "def select_last_inserted_table_id(cursor, table_name, table_id_type):\n",
    "    \"\"\"Select the last inserted row.\"\"\"\n",
    "\n",
    "    last_id = get_last_row_id(cursor, table_name)\n",
    "    last_entry = cursor.execute(\"SELECT {} FROM {} WHERE rowid = {}\".format(table_id_type, table_name, last_id))\n",
    "    last_entry = last_entry.fetchone()\n",
    "\n",
    "    return last_entry[0]\n",
    "\n",
    "\n",
    "def select_last_inserted_table_row(cursor, table_name):\n",
    "    \"\"\"Select the last inserted row.\"\"\"\n",
    "\n",
    "    last_id = get_last_row_id(cursor, table_name)\n",
    "    last_entry = cursor.execute(\"SELECT * FROM {} WHERE rowid = {}\".format(table_name, last_id))\n",
    "    last_entry = last_entry.fetchone()\n",
    "\n",
    "    return last_entry\n",
    "\n",
    "\n",
    "def make_new_id(id_string):\n",
    "    \"\"\"Convert old string id to old string id + 1.\"\"\"\n",
    "\n",
    "    new_id = int(id_string) + 1\n",
    "    return str(new_id)\n",
    "\n",
    "\n",
    "def check_species_name_in_database(cursor, species_name):\n",
    "    \"\"\"Safely return species id or None.\"\"\"\n",
    "    try:\n",
    "        species_id = cursor.execute(\"SELECT spe_id FROM Species WHERE name = '%s'\" % species_name.lower())\n",
    "        species_id = species_id.fetchone()[0]\n",
    "        return species_id\n",
    "\n",
    "    except TypeError:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def make_sbol_string_db_update(input_list, direction):\n",
    "    \"\"\" Make an sbol string using uploading information.\"\"\"\n",
    "    color_list = np.linspace(1, 14, 14)\n",
    "    random_color_list = []\n",
    "    for next in color_list:\n",
    "        random_color_list.append(int(next))\n",
    "\n",
    "    if direction.lower() == 'l':\n",
    "        direction = '<'\n",
    "    else:\n",
    "        direction = ''\n",
    "    output_string = ''\n",
    "    for species in input_list:\n",
    "        first_character = species[0]\n",
    "        species = species[1::]\n",
    "\n",
    "        if first_character == 'p':\n",
    "            output_string = output_string + direction + 'p ' + species + ' ' + str(\n",
    "                random_color_list.pop(random.randrange(0, len(random_color_list)))) + '\\n'\n",
    "\n",
    "        else:\n",
    "            output_string = output_string + direction + 'c ' + species + ' ' + str(\n",
    "                random_color_list.pop(random.randrange(0, len(random_color_list)))) + '\\n'\n",
    "\n",
    "    output_string = output_string + direction + 't ' + str(\n",
    "        random_color_list.pop(random.randrange(0, len(random_color_list)))) + '\\n# Arcs'\n",
    "\n",
    "    return output_string\n",
    "\n",
    "\n",
    "def make_sbol_file(output_species_list, promoter_list, operon_direction, operon_id, path_directory):\n",
    "    \"\"\"Insert and make the sbol file.\"\"\"\n",
    "\n",
    "    sbol_file = path_directory + \"/pigeonImages/operon_sbol_\" + operon_id + \".txt\"\n",
    "    sbol_list = promoter_list + [\"c\" + data[0] for data in output_species_list]\n",
    "    sbol_string = make_sbol_string_db_update(sbol_list, operon_direction)\n",
    "    sbol_handle = open(sbol_file, 'w')\n",
    "    sbol_handle.write(sbol_string)\n",
    "    sbol_handle.close()\n",
    "    return sbol_file\n",
    "\n",
    "\n",
    "def make_input_transition_sbml_file(input_species_list, transition_id, operon_id, trans_logic):\n",
    "    input_species_id_repression_list = [(\"spe_\" + data[0], data[2]) for data in input_species_list]\n",
    "    input_transition_sbml_list = [operon_id] + input_species_id_repression_list\n",
    "    it_sbml_file_name = rootPath + \"/SBML_TXT_FILES/it_sbml_{}\".format(transition_id)\n",
    "    su.sbml_input_trans(transition_id,\n",
    "                        input_species_id_repression_list,\n",
    "                        \"ope_\" + operon_id,\n",
    "                        trans_logic,\n",
    "                        it_sbml_file_name)\n",
    "\n",
    "\n",
    "def make_output_transition_sbml_file(output_species_list, transition_id, operon_id):\n",
    "    \"\"\"make the sbml.\"\"\"\n",
    "    os_abbrev_id_list = [\"spe_\" + data[-1] for data in output_species_list]\n",
    "    ot_sbml_file_name = rootPath + \"/SBML_TXT_FILES/ot_sbml_{}\".format(transition_id)\n",
    "    su.sbml_output_trans(transition_id,\n",
    "                         os_abbrev_id_list,\n",
    "                         \"ope_\" + operon_id,\n",
    "                         ot_sbml_file_name)\n",
    "\n",
    "\n",
    "def insert_new_plasmid(cursor, plasmid_name, PMID):\n",
    "    \"\"\"Insert new plasmid.\"\"\"\n",
    "    plasmid_id = select_last_inserted_table_id(cursor, \"Plasmid\", \"pla_id\")\n",
    "    plasmid_id = make_new_id(plasmid_id)\n",
    "    db.db_insert(cursor, \"Plasmid\", [\"pla_id\", \"name\", \"PMID\"], [plasmid_id, plasmid_name, PMID])\n",
    "    return plasmid_id\n",
    "\n",
    "\n",
    "def insert_new_operon(cursor, plasmid_id, operon_name, direction):\n",
    "    \"\"\"Insert new operon.\"\"\"\n",
    "    operon_id = select_last_inserted_table_id(cursor, \"Operon\", \"ope_id\").replace(\"-\", \"\")\n",
    "    operon_id = make_new_id(operon_id)\n",
    "    sbol = \"operon_sbol_{}.png\".format(operon_id)\n",
    "    sbml = \"operon_sbml_{}.txt\".format(operon_id)\n",
    "    db.db_insert(cursor, \"PlasmidOperon\", [\"ope_id\", \"pla_id\", \"direction\"], [operon_id, plasmid_id, direction])\n",
    "    db.db_insert(cursor, \"Operon\", [\"ope_id\", \"name\", \"sbol\", \"sbml\"], [operon_id, operon_name, sbol, sbml])\n",
    "    return operon_id\n",
    "\n",
    "\n",
    "def insert_new_input_transition(cursor, operon_id, logic):\n",
    "    \"\"\"Insert new input transition.\"\"\"\n",
    "\n",
    "    it_id = select_last_inserted_table_id(cursor, \"InputTransition\", \"it_id\")\n",
    "    it_id = make_new_id(it_id)\n",
    "    sbml = \"it_sbml_{}.txt\".format(it_id)\n",
    "    db.db_insert(cursor, \"OperonInputTransition\", [\"ope_id\", \"it_id\"], [operon_id, it_id])\n",
    "    db.db_insert(cursor, \"InputTransition\", [\"it_id\", \"logic\", \"sbml\"], [it_id, logic, sbml])\n",
    "\n",
    "    return it_id\n",
    "\n",
    "\n",
    "def insert_new_input_transition_species(cursor, it_id, species_name, species_type, species_repression):\n",
    "    \"\"\"Insert new input transition species.\"\"\"\n",
    "\n",
    "    check_db_species_id = check_species_name_in_database(cursor, species_name)\n",
    "    if check_db_species_id == \"\":\n",
    "        last_spe_id = select_last_inserted_table_id(cursor, \"Species\", \"spe_id\")\n",
    "        spe_id = make_new_id(last_spe_id)\n",
    "        sbml = \"species_sbml_{}\".format(spe_id)\n",
    "        db.db_insert(cursor, \"Species\", [\"spe_id\", \"name\", \"type\", \"sbml\"],\n",
    "                     [spe_id, species_name.lower(), species_type.lower(), sbml])\n",
    "        sbml_species_file = rootPath + \"/SBML_TXT_FILES/\" + sbml\n",
    "        su.sbml_species(it_id, species_name, sbml_species_file)\n",
    "\n",
    "    else:\n",
    "        spe_id = check_db_species_id\n",
    "    last_in_id = select_last_inserted_table_id(cursor, \"InputTransitionSpecies\", \"in_id\")\n",
    "    in_id = make_new_id(last_in_id)\n",
    "    db.db_insert(cursor, \"InputTransitionSpecies\", [\"in_id\", \"it_id\", \"spe_id\", \"repression\"],\n",
    "                 [in_id, it_id, spe_id, species_repression])\n",
    "\n",
    "    return spe_id\n",
    "\n",
    "\n",
    "def insert_new_output_transition(cursor, operon_id):\n",
    "    \"\"\"Insert new output transition.\"\"\"\n",
    "    ot_id = select_last_inserted_table_id(cursor, \"OutputTransition\", \"ot_id\")\n",
    "    ot_id = make_new_id(ot_id)\n",
    "    sbml = \"ot_sbml_{}.txt\".format(ot_id)\n",
    "    db.db_insert(cursor, \"OperonOutputTransition\", [\"ope_id\", \"ot_id\"], [operon_id, ot_id])\n",
    "    db.db_insert(cursor, \"OutputTransition\", [\"ot_id\", \"sbml\"], [ot_id, sbml])\n",
    "\n",
    "    return ot_id\n",
    "\n",
    "\n",
    "def insert_new_output_transition_species(cursor, ot_id, species_name, species_type):\n",
    "    \"\"\"Insert new output transition species.\"\"\"\n",
    "\n",
    "    check_db_species_id = check_species_name_in_database(cursor, species_name.strip().lower())\n",
    "    if check_db_species_id == \"\":\n",
    "        last_spe_id = select_last_inserted_table_id(cursor, \"Species\", \"spe_id\")\n",
    "        spe_id = make_new_id(last_spe_id)\n",
    "        sbml = \"species_sbml_{}\".format(spe_id)\n",
    "        sbml_species_file = rootPath + \"/SBML_TXT_FILES/\" + sbml\n",
    "        db.db_insert(cursor, \"Species\", [\"spe_id\", \"name\", \"type\", \"sbml\"],\n",
    "                     [spe_id, species_name.lower(), species_type.lower(), sbml])\n",
    "        su.sbml_species(ot_id, species_name, sbml_species_file)\n",
    "\n",
    "    else:\n",
    "        spe_id = check_db_species_id\n",
    "\n",
    "    last_out_id = select_last_inserted_table_id(cursor, \"OutputTransitionSpecies\", \"out_id\")\n",
    "    out_id = make_new_id(last_out_id)\n",
    "    db.db_insert(cursor, \"OutputTransitionSpecies\", [\"out_id\", \"ot_id\", \"spe_id\"], [out_id, ot_id, spe_id])\n",
    "\n",
    "    return spe_id\n",
    "\n",
    "\n",
    "def get_data_keyword(data_string):\n",
    "    \"\"\"Get the keyword belonging to data_string.\"\"\"\n",
    "\n",
    "    colon_index = data_string.index(\":\")\n",
    "    return data_string[0:colon_index:]\n",
    "\n",
    "\n",
    "def determine_parent_keyword(component_keyword):\n",
    "    \"\"\"Determine which is your parent keyword.\"\"\"\n",
    "\n",
    "    if component_keyword == \"Operon\":\n",
    "        return \"Plasmid\"\n",
    "\n",
    "    elif component_keyword == \"InputTransition\":\n",
    "        return \"Operon\"\n",
    "\n",
    "    elif component_keyword == \"InputSpecies\":\n",
    "        return \"InputTransition\"\n",
    "\n",
    "    elif component_keyword == \"OutputTransition\":\n",
    "        return \"Operon\"\n",
    "\n",
    "    elif component_keyword == \"OutputSpecies\":\n",
    "        return \"OutputTransition\"\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def determine_and_insert(connection, cursor, component_keyword, component_data=[], parent_component_id=\"\"):\n",
    "    \"\"\"Determine insertion method and insert into into the database.\"\"\"\n",
    "\n",
    "    if component_keyword == \"Plasmid\":\n",
    "        data_id = insert_new_plasmid(cursor, *component_data)\n",
    "\n",
    "    elif component_keyword == \"Operon\":\n",
    "        data_id = insert_new_operon(cursor, parent_component_id, *component_data)\n",
    "        operon_sbml = rootPath + \"/SBML_TXT_FILES/operon_sbml_{}\".format(data_id)\n",
    "        su.sbml_operon(data_id, component_data[0], data_id, operon_sbml)\n",
    "\n",
    "    elif component_keyword == \"InputTransition\":\n",
    "        data_id = insert_new_input_transition(cursor, parent_component_id, *component_data)\n",
    "\n",
    "    elif component_keyword == \"InputSpecies\":\n",
    "        data_id = insert_new_input_transition_species(cursor, parent_component_id, *component_data)\n",
    "\n",
    "    elif component_keyword == \"OutputTransition\":\n",
    "        data_id = insert_new_output_transition(cursor, parent_component_id)\n",
    "\n",
    "    elif component_keyword == \"OutputSpecies\":\n",
    "        data_id = insert_new_output_transition_species(cursor, parent_component_id, *component_data)\n",
    "\n",
    "    else:\n",
    "        data_id = \"\"\n",
    "\n",
    "    connection.commit()\n",
    "    return data_id\n",
    "\n",
    "\n",
    "def insert_new_device(connection, cursor, device, path_directory):\n",
    "    \"\"\"Inserts a new device into the database.\n",
    "        Argument(s):\n",
    "            connection - sqlite3 connection object\n",
    "            cursor - sqlite3 cursor object\n",
    "            device_string - whole device as a string\n",
    "    \"\"\"\n",
    "\n",
    "    parent_ids_dict = {\"Plasmid\": \"\", \"Operon\": \"\", \"InputTransition\": \"\", \"OutputTransition\": \"\"}\n",
    "    input_species_list = []\n",
    "    output_species_list = []\n",
    "    promoter_list = []\n",
    "    sbol_files = []\n",
    "\n",
    "    for component in device:\n",
    "        component_keyword = get_data_keyword(component)\n",
    "        component_data = component.replace(component_keyword + \":\", \"\")\n",
    "        component_data = component_data.split(\",\")\n",
    "\n",
    "        if component_keyword == \"Operon\" and len(output_species_list) > 0:\n",
    "\n",
    "            sbol_files.append(\n",
    "                make_sbol_file(output_species_list, promoter_list,\\\n",
    "                    prev_operon_direction, parent_ids_dict[\"Operon\"]), path_directory)\n",
    "            prev_operon_direction = component_data[:-1:][0]\n",
    "\n",
    "\n",
    "            # ##sbml input transition file creation\n",
    "            make_input_transition_sbml_file(input_species_list, parent_ids_dict[\"InputTransition\"],\n",
    "                                            parent_ids_dict[\"Operon\"], input_trans_logic)\n",
    "\n",
    "            # ##sbml output transition file creation\n",
    "            make_output_transition_sbml_file(output_species_list, parent_ids_dict[\"OutputTransition\"],\n",
    "                                             parent_ids_dict[\"Operon\"])\n",
    "\n",
    "            input_species_list = []\n",
    "            output_species_list = []\n",
    "            promoter_list = []\n",
    "\n",
    "        # ##Capturing the previous operon's direction for sbol\n",
    "        elif component_keyword == \"Operon\" and len(output_species_list) == 0:\n",
    "            prev_operon_direction = component_data[:-1:][0]\n",
    "\n",
    "        # ##Capturing the input transition logic for sbml\n",
    "        if component_keyword == \"InputTransition\":\n",
    "            input_trans_logic = component_data[0]\n",
    "\n",
    "\n",
    "        # Update the database exclusively except for 4th level conditions\n",
    "        if component_keyword != \"Plasmid\":\n",
    "\n",
    "            # Covering the insert for input transitions, operons, and output transitions\n",
    "            if component_keyword in [\"InputTransition\", \"Operon\", \"OutputTransition\"]:\n",
    "                parent_keyword = determine_parent_keyword(component_keyword)\n",
    "                parent_id = parent_ids_dict[parent_keyword]\n",
    "                #if component_keyword == \"InputTransition\":\n",
    "                #promoter_list.append(\"p\" + component_data[0])\n",
    "                #component_data = component_data[1::]\n",
    "                component_id = determine_and_insert(connection, cursor, component_keyword, component_data, parent_id)\n",
    "                parent_ids_dict[component_keyword] = component_id\n",
    "\n",
    "            #keeping track of output and input transition information that will be used for the sbol image and as\n",
    "            #as well as input and output transition sbml txt.\n",
    "            elif component_keyword in [\"InputSpecies\", \"OutputSpecies\", \"Promoter\"]:\n",
    "                #component_id = determine_and_insert(connection, cursor, component_keyword, component_data)\n",
    "\n",
    "                if component_keyword == \"InputSpecies\":\n",
    "                    input_species = list(component_data) + [component_id]\n",
    "                    input_species_list.append(input_species)\n",
    "                    parent_keyword = determine_parent_keyword(component_keyword)\n",
    "                    parent_id = parent_ids_dict[parent_keyword]\n",
    "                    component_id = determine_and_insert(connection, cursor, component_keyword, component_data,\n",
    "                                                        parent_id)\n",
    "\n",
    "                elif component_keyword == \"OutputSpecies\":\n",
    "\n",
    "                    ###Capturing the outputtransion\n",
    "                    if len(output_species_list) == 0:\n",
    "                        parent_id = parent_ids_dict[\"Operon\"]\n",
    "                        component_id = determine_and_insert(connection, cursor, \"OutputTransition\", [], parent_id)\n",
    "                        parent_ids_dict[\"OutputTransition\"] = component_id\n",
    "\n",
    "                    output_species = list(component_data) + [component_id]\n",
    "                    output_species_list.append(output_species)\n",
    "                    parent_keyword = determine_parent_keyword(component_keyword)\n",
    "                    parent_id = parent_ids_dict[parent_keyword]\n",
    "                    component_id = determine_and_insert(connection, cursor, component_keyword, component_data,\n",
    "                                                        parent_id)\n",
    "                else:\n",
    "                    promoter_list.append(\"p\" + component_data[0])\n",
    "        else:\n",
    "            plasmid_id = determine_and_insert(connection, cursor, component_keyword, component_data)\n",
    "            parent_ids_dict[\"Plasmid\"] = plasmid_id\n",
    "\n",
    "\n",
    "    # ##Covering the last sbol that has to be created. It would be skipped over because it is the last one otherwise.\n",
    "    sbol_files.append(\n",
    "        make_sbol_file(output_species_list, promoter_list, prev_operon_direction, parent_ids_dict[\"Operon\"], path_directory))\n",
    "\n",
    "    make_input_transition_sbml_file(input_species_list, parent_ids_dict[\"InputTransition\"], parent_ids_dict[\"Operon\"],\n",
    "                                    input_trans_logic)\n",
    "\n",
    "    make_output_transition_sbml_file(output_species_list, parent_ids_dict[\"OutputTransition\"],\n",
    "                                     parent_ids_dict[\"Operon\"])\n",
    "\n",
    "    connection.commit()\n",
    "    return \",\".join(sbol_files)\n",
    "\n",
    "def main():\n",
    "    web_path = sys.argv[1]\n",
    "    database_file = web_path + \"sbider.db\"\n",
    "    global rootPath \n",
    "    rootPath = web_path\n",
    "    device_info = sys.argv[2::]\n",
    "    conn, cur = db.db_open(database_file)\n",
    "    sbol_files = insert_new_device(conn, cur, device_info, web_path)\n",
    "    db.db_close(conn, cur)\n",
    "\n",
    "    #conn, cur = db.db_open(database_file)\n",
    "    #sg.create_network_json_file(cur, web_path + \"whole_network.json\")\n",
    "    #db.db_close(conn, cur)\n",
    "    #gn.create_whole_network_sbml()\n",
    "    \n",
    "    return sbol_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## access database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def db_print_table(cursor, table_name):\n",
    "    \"\"\"\n",
    "    Print a table.\n",
    "    \"\"\"\n",
    "\n",
    "    cursor.execute(\"SELECT * FROM \" + table_name)\n",
    "    table = cursor.fetchall()\n",
    "    print(table_name + ' table:')\n",
    "    for l in table:\n",
    "        print(l)\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "def db_print_all_tables(cursor):\n",
    "    \"\"\"\n",
    "    Print all database tables.\n",
    "    \"\"\"\n",
    "    db_print_table(cursor, \"Species\")\n",
    "    db_print_table(cursor, \"Plasmid\")\n",
    "    db_print_table(cursor, \"Operon\")\n",
    "    db_print_table(cursor, \"PlasmidOperon\")\n",
    "    db_print_table(cursor, \"OperonInputTransition\")\n",
    "    db_print_table(cursor, \"InputTransition\")\n",
    "    db_print_table(cursor, \"InputTransitionSpecies\")\n",
    "    db_print_table(cursor, \"OperonOutputTransition\")    \n",
    "    db_print_table(cursor, \"OutputTransition\")\n",
    "    db_print_table(cursor, \"OutputTransitionSpecies\")\n",
    "    db_print_table(cursor, \"User\")\n",
    "\n",
    "    \n",
    "def db_insert(cursor, table_name, table_header_list, insert_data_list):\n",
    "    \"\"\"\n",
    "    Insert into a SQL table.\n",
    "    \"\"\"\n",
    "\n",
    "    sql_command = make_sql_insert_command(table_name, table_header_list, insert_data_list)\n",
    "    cursor.execute(sql_command)\n",
    "    return cursor\n",
    "\n",
    "\n",
    "def db_select(cursor, table_name, table_header_list, where_columns=None, where_options=None,\n",
    "                  where_values=None, where_bools=None, group=None, having_columns=None, having_bools=None,\n",
    "                  having_values=None):\n",
    "    \"\"\"\n",
    "    Select from a SQL table.\n",
    "    \"\"\"\n",
    "\n",
    "    sql_command = make_sql_select_command(table_name, table_header_list, where_columns, where_options,\n",
    "                                          where_values, where_bools, group, having_columns, having_bools, having_values)\n",
    "    cursor.execute(sql_command)\n",
    "    return cursor\n",
    "\n",
    "\n",
    "def db_update(cursor, table_name, table_header_list, update_data_list,\n",
    "              where_column=\"\", where_option=\"\", where_value=\"\"):\n",
    "    \"\"\"\n",
    "    Update a SQL table.\n",
    "    \"\"\"\n",
    "\n",
    "    sql_command = make_sql_update_command(table_name, table_header_list, update_data_list,\n",
    "                                          where_column, where_option, where_value)\n",
    "    cursor.execute(sql_command)\n",
    "    return cursor\n",
    "\n",
    "\n",
    "def db_delete(cursor, table_name):\n",
    "    \"\"\"\n",
    "    Delete a SQL table.\n",
    "    \"\"\"\n",
    "    cursor.execute(make_sql_delete_command(table_name))\n",
    "    \n",
    "    \n",
    "def db_get_species_id_from_name(cursor, species_name):\n",
    "    \"\"\"\n",
    "    Get species id from species name.\n",
    "    \"\"\"\n",
    "    \n",
    "    a_cur = db_select(cursor,\n",
    "                      \"Species\",\n",
    "                      [\"spe_id\"],\n",
    "                      [\"name\"],\n",
    "                      [\"=\"],\n",
    "                      [\"'%s'\" % species_name.lower()],\n",
    "                      \"\")\n",
    "    return a_cur.fetchone()[0]\n",
    "\n",
    "\n",
    "def db_get_species_name_from_id(cursor, species_id):\n",
    "    \"\"\"\n",
    "    Get species name from species id.\n",
    "    \"\"\"\n",
    "    \n",
    "    a_cur = db_select(cursor,\n",
    "                      \"Species\",\n",
    "                      [\"name\"],\n",
    "                      [\"spe_id\"],\n",
    "                      [\"=\"],\n",
    "                      [species_id],\n",
    "                      \"\")\n",
    "    return a_cur.fetchone()[0]\n",
    "\n",
    "\n",
    "def db_get_operon_id_from_name(cursor, operon_name):\n",
    "    \"\"\"\n",
    "    Get operon id from operon name.\n",
    "    \"\"\"\n",
    "    \n",
    "    a_cur = db_select(cursor,\n",
    "                      \"Operon\",\n",
    "                      [\"ope_id\"],\n",
    "                      [\"name\"],\n",
    "                      [\"=\"],\n",
    "                      [\"'%s'\" % operon_name],\n",
    "                      \"\")\n",
    "    return a_cur.fetchone()[0]\n",
    "\n",
    "\n",
    "def db_get_operon_name_from_id(cursor, operon_id):\n",
    "    \"\"\"\n",
    "    Get operon name from operon id.\n",
    "    \"\"\"\n",
    "    \n",
    "    a_cur = db_select(cursor,\n",
    "                      \"Operon\",\n",
    "                      [\"name\"],\n",
    "                      [\"ope_id\"],\n",
    "                      [\"=\"],\n",
    "                      [\"'%s'\" % operon_id],\n",
    "                      \"\")\n",
    "    return a_cur.fetchone()[0]\n",
    "\n",
    "\n",
    "def db_get_plasmid_id_from_name(cursor, plasmid_name):\n",
    "    \"\"\"\n",
    "    Get plasmid id from plasmid name.\n",
    "    \"\"\"\n",
    "    \n",
    "    a_cur = db_select(cursor,\n",
    "                      \"Plasmid\",\n",
    "                      [\"pla_id\"],\n",
    "                      [\"name\"],\n",
    "                      [\"=\"],\n",
    "                      [\"'%s'\" % plasmid_name.lower()],\n",
    "                      \"\")\n",
    "    return a_cur.fetchone()[0]\n",
    "\n",
    "\n",
    "def db_get_plasmid_name_from_id(cursor, plasmid_id):\n",
    "    \"\"\"\n",
    "    Get plasmid name from plasmid id.\n",
    "    \"\"\"\n",
    "    \n",
    "    a_cur = db_select(cursor,\n",
    "                      \"Plasmid\",\n",
    "                      [\"name\"],\n",
    "                      [\"pla_id\"],\n",
    "                      [\"=\"],\n",
    "                      [plasmid_id],\n",
    "                      \"\")\n",
    "    return a_cur.fetchone()[0]\n",
    "\n",
    "\n",
    "def get_all_input_transition_species(cursor, input_transition_id):\n",
    "    \"\"\"\n",
    "    Get all species an input transition requires for activation.\n",
    "    \"\"\"\n",
    "\n",
    "    species_list = []\n",
    "    species_list_unformatted = db_select(cursor,\n",
    "                                         \"InputTransitionSpecies\",\n",
    "                                         [\"spe_id\"],\n",
    "                                         [\"it_id\"],\n",
    "                                         [\"=\"],\n",
    "                                         [\"'\" + input_transition_id + \"'\"],\n",
    "                                         [\"\"])\n",
    "    species_list_unformatted = species_list_unformatted.fetchall()\n",
    "    \n",
    "    for species_index in range(len(species_list_unformatted)):\n",
    "        species_list.append(list(species_list_unformatted[species_index]))\n",
    "        \n",
    "    species_list = helper.uniquely_merge_multi_dimensional_list_of_lists(species_list)\n",
    "    return species_list\n",
    "\n",
    "\n",
    "def get_all_output_transition_species(cursor, input_transition_id):\n",
    "    \"\"\"\n",
    "    Get all species an output transition produces.\n",
    "    \"\"\"\n",
    "    \n",
    "    species_list = []\n",
    "    species_list_unformatted = db_select(cursor,\n",
    "                                         \"OutputTransitionSpecies\",\n",
    "                                         [\"spe_id\"],\n",
    "                                         [\"ot_id\"],\n",
    "                                         [\"=\"],\n",
    "                                         [\"'\" + input_transition_id + \"'\"],\n",
    "                                         [\"\"])\n",
    "    species_list_unformatted = species_list_unformatted.fetchall()\n",
    "\n",
    "    for species_index in range(len(species_list_unformatted)):\n",
    "        species_list.append(list(species_list_unformatted[species_index]))\n",
    "\n",
    "    species_list = helper.uniquely_merge_multi_dimensional_list_of_lists(species_list)\n",
    "    return species_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_input_ope_id_spe_id_dic(cursor):\n",
    "    \"\"\"\n",
    "    Make operon input species dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    input_ope_id_spe_id_dict = {}\n",
    "\n",
    "    merged_ope_it_spe = cursor.execute('''SELECT OperonInputTransition.ope_id, OperonInputTransition.it_id, InputTransitionSpecies.spe_id \n",
    "                                          FROM OperonInputTransition, InputTransitionSpecies \n",
    "                                          WHERE OperonInputTransition.it_id = InputTransitionSpecies.it_id''')\n",
    "\n",
    "    previous_operon, previous_input_transition, previous_species = merged_ope_it_spe.fetchone()\n",
    "    \n",
    "    input_transition_list_idx = 0\n",
    "    \n",
    "    input_ope_id_spe_id_dict[previous_operon] = [[]]\n",
    "    input_ope_id_spe_id_dict[previous_operon][input_transition_list_idx].append(previous_species.strip())\n",
    "\n",
    "    for operon, input_transition, species in merged_ope_it_spe.fetchall():\n",
    "        \n",
    "        if operon == previous_operon:\n",
    "            if input_transition == previous_input_transition:\n",
    "                input_ope_id_spe_id_dict[operon][input_transition_list_idx].append(species.strip())\n",
    "            elif input_transition != previous_input_transition:\n",
    "                input_transition_list_idx += 1\n",
    "                input_ope_id_spe_id_dict[operon].append([])\n",
    "                input_ope_id_spe_id_dict[operon][input_transition_list_idx].append(species.strip())\n",
    "                previous_input_transition = input_transition\n",
    "        else:\n",
    "            input_transition_list_idx = 0\n",
    "            input_ope_id_spe_id_dict[operon] = [[]]\n",
    "            input_ope_id_spe_id_dict[operon][input_transition_list_idx].append(species.strip())\n",
    "            previous_operon = operon\n",
    "            previous_input_transition = input_transition\n",
    "\n",
    "    return input_ope_id_spe_id_dict\n",
    "\n",
    "\n",
    "def make_output_ope_id_spe_id_dic(cursor):\n",
    "    \"\"\"\n",
    "    Make operon output species dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    output_ope_id_spe_id_dict = {}\n",
    "\n",
    "    merged_ope_ot_spe = cursor.execute('''SELECT OperonOutputTransition.ope_id, OperonOutputTransition.ot_id, OutputTransitionSpecies.spe_id \n",
    "                                          FROM OperonOutputTransition, OutputTransitionSpecies \n",
    "                                          WHERE OperonOutputTransition.ot_id = OutputTransitionSpecies.ot_id''')\n",
    "\n",
    "    # previous ope_id, ot_id, and spe_id\n",
    "    previous_operon, previous_output_transition, previous_species = merged_ope_ot_spe.fetchone()\n",
    "\n",
    "    output_transition_list_idx = 0\n",
    "\n",
    "    output_ope_id_spe_id_dict[previous_operon] = [[]]\n",
    "    output_ope_id_spe_id_dict[previous_operon][output_transition_list_idx].append(previous_species.strip())\n",
    "\n",
    "    # ope_id, ot_id, and spe_id\n",
    "    for operon, output_transition, species in merged_ope_ot_spe.fetchall():\n",
    "\n",
    "        if operon == previous_operon and not helper.contain_all_elements(output_ope_id_spe_id_dict[operon], [species]):\n",
    "            if output_transition == previous_output_transition:\n",
    "                output_ope_id_spe_id_dict[operon][output_transition_list_idx].append(species.strip())\n",
    "            else:\n",
    "                output_transition_list_idx += 1\n",
    "                output_ope_id_spe_id_dict[operon].append([])\n",
    "                output_ope_id_spe_id_dict[operon][output_transition_list_idx].append(species.strip())\n",
    "        else:\n",
    "            output_transition_list_idx = 0\n",
    "            output_ope_id_spe_id_dict[operon] = [[]]\n",
    "            output_ope_id_spe_id_dict[operon][output_transition_list_idx].append(species.strip())\n",
    "            previous_operon = operon\n",
    "            previous_output_transition = output_transition\n",
    "\n",
    "    return output_ope_id_spe_id_dict\n",
    "\n",
    "\n",
    "def make_ope_id_spe_id_dics(cursor):\n",
    "    \"\"\"\n",
    "    Make operon input species and operon output species dictionaries.\n",
    "    \"\"\"\n",
    "    \n",
    "    return make_input_ope_id_spe_id_dic(cursor), make_output_ope_id_spe_id_dic(cursor)\n",
    "\n",
    "\n",
    "def make_ope_id_rep_spe_id_dic(cursor):\n",
    "    \"\"\"\n",
    "    Make operon input-not dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    input_ope_id_spe_not_dict = {}\n",
    "    merged_ope_it_spe_not = cursor.execute('''SELECT OperonInputTransition.ope_id, OperonInputTransition.it_id, InputTransitionSpecies.spe_id, InputTransitionSpecies.repression\n",
    "                                              FROM OperonInputTransition, InputTransitionSpecies \n",
    "                                              WHERE OperonInputTransition.it_id = InputTransitionSpecies.it_id''')\n",
    "\n",
    "    previous_operon, previous_input_transition, previous_species, not_bool = merged_ope_it_spe_not.fetchone()\n",
    "\n",
    "    input_transition_list_idx = 0\n",
    "\n",
    "    input_ope_id_spe_not_dict[previous_operon] = [[]]\n",
    "\n",
    "    if not_bool == \"TRUE\":\n",
    "        input_ope_id_spe_not_dict[previous_operon][input_transition_list_idx].append(previous_species.strip())\n",
    "\n",
    "    for operon, input_transition, species, not_bool in merged_ope_it_spe_not.fetchall():\n",
    "        if operon == previous_operon:\n",
    "            \n",
    "            if not_bool == \"TRUE\":\n",
    "                if input_transition == previous_input_transition:\n",
    "                    input_ope_id_spe_not_dict[operon][input_transition_list_idx].append(species.strip())\n",
    "                elif input_transition != previous_input_transition:\n",
    "                    input_transition_list_idx += 1\n",
    "                    input_ope_id_spe_not_dict[operon].append([])\n",
    "                    input_ope_id_spe_not_dict[operon][input_transition_list_idx].append(species.strip())\n",
    "                    previous_input_transition = input_transition\n",
    "\n",
    "        else:\n",
    "            input_transition_list_idx = 0\n",
    "            input_ope_id_spe_not_dict[operon] = [[]]\n",
    "            if not_bool == \"TRUE\":\n",
    "                input_ope_id_spe_not_dict[operon][input_transition_list_idx].append(species.strip())\n",
    "            previous_operon = operon\n",
    "            previous_input_transition = input_transition\n",
    "\n",
    "    return input_ope_id_spe_not_dict\n",
    "\n",
    "\n",
    "def make_plasmid_species_name_dictionary(cursor, operon_id_plasmid_name_dictionary, operon_species_dictionary):\n",
    "    plasmid_species_name_dictionary = {}\n",
    "    for operon_id, species_id_list in operon_species_dictionary.items():\n",
    "        uniquely_merge_spe_id_list = helper.uniquely_merge_multi_dimensional_list_of_lists(species_id_list)\n",
    "        plasmid_name = operon_id_plasmid_name_dictionary[operon_id]\n",
    "        plasmid_species_name_dictionary[plasmid_name] = [db_get_species_name_from_id(cursor, spe_id) for spe_id in\n",
    "                                                         uniquely_merge_spe_id_list]\n",
    "    return plasmid_species_name_dictionary\n",
    "\n",
    "\n",
    "def make_pla_name_spe_name_dics(cursor):\n",
    "    \"\"\"Make operon input and output species dictionary.\"\"\"\n",
    "\n",
    "    #plasmid_name_input_species_name_dictionary = {}\n",
    "    #plasmid_name_output_species_name_dictionary = {}\n",
    "\n",
    "    operon_id_plasmid_name_dictionary = {}\n",
    "    #species_id_to_name_dictionary = {}\n",
    "\n",
    "    input_operon_species_dictionary, output_operon_species_dictionary = make_ope_id_spe_id_dics(cursor)\n",
    "\n",
    "    # make operon_id plasmid_name dictionary\n",
    "    merged_ope_id_pla_name = cursor.execute('''SELECT PlasmidOperon.ope_id,\n",
    "                                                   Plasmid.name\n",
    "                                            FROM PlasmidOperon,\n",
    "                                                 Plasmid\n",
    "                                            WHERE PlasmidOperon.pla_id = Plasmid.pla_id''')\n",
    "    for ope_id, pla_name in merged_ope_id_pla_name.fetchall():\n",
    "        operon_id_plasmid_name_dictionary[ope_id] = pla_name\n",
    "\n",
    "    input_plasmid_species_name_dictionary = make_plasmid_species_name_dictionary(cursor,\n",
    "                                                                                 operon_id_plasmid_name_dictionary,\n",
    "                                                                                 input_operon_species_dictionary)\n",
    "    output_plasmid_species_name_dictionary = make_plasmid_species_name_dictionary(cursor,\n",
    "                                                                                  operon_id_plasmid_name_dictionary,\n",
    "                                                                                  output_operon_species_dictionary)\n",
    "    return input_plasmid_species_name_dictionary, output_plasmid_species_name_dictionary\n",
    "\n",
    "\n",
    "def operon_PMC_dictionary(database):\n",
    "    conn, cur = db_open(database)\n",
    "    operon_PMC_dict = {}\n",
    "    operon_PMC = cur.execute('''SELECT PlasmidOperon.ope_id, Plasmid.PMID\n",
    "                                FROM PlasmidOperon, Plasmid\n",
    "                                WHERE PlasmidOperon.pla_id == Plasmid.pla_id''')\n",
    "    \n",
    "    for ope_id, PMC_ID in operon_PMC:\n",
    "        operon_PMC_dict[ope_id] = PMC_ID.replace(' PMID: ', '')\n",
    "    db_close(conn, cur)\n",
    "    return operon_PMC_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input_analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grammar_0(cursor, tokens):\n",
    "    \"\"\"\n",
    "    Grammar for 'grammar_0:= grammar_1 = grammar_1'.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(tokens) < 3 or '=' not in tokens:\n",
    "        raise ValueError(\"grammar_0: Usage: input = output\")\n",
    "        \n",
    "    else:\n",
    "        input_output_dictionary = helper.split_by(tokens, '=')\n",
    "\n",
    "    return grammar_output(grammar_1(cursor, input_output_dictionary[0]), \n",
    "                          grammar_1(cursor, input_output_dictionary[1]))\n",
    "\n",
    "\n",
    "def grammar_1(cursor, tokens):\n",
    "    \"\"\"\n",
    "    Grammar for 'grammar_1:= grammar_2 or grammar_1 | grammar_2 and grammar_1 | grammar_2'.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(tokens) > 1 and tokens[1] == 'or':\n",
    "        # grammar_2 or grammar_1\n",
    "        # split tokens by the first occurring 'or' and store the tokens before\n",
    "        # and after the 'or' in a dictionary\n",
    "        or_dictionary = helper.split_by(tokens, 'or')\n",
    "        return grammar_or(grammar_2(cursor, or_dictionary.get(0)), grammar_1(cursor, or_dictionary.get(1)))\n",
    "\n",
    "    elif len(tokens) > 1 and tokens[1] == 'and':\n",
    "        # grammar_2 and grammar_1\n",
    "        # split tokens by the first occurring 'and' and store the tokens\n",
    "        # before and after the 'and' in a dictionary\n",
    "        and_dictionary = helper.split_by(tokens, 'and')\n",
    "        return grammar_and(grammar_2(cursor, and_dictionary.get(0)), grammar_1(cursor, and_dictionary.get(1)))\n",
    "\n",
    "    else:\n",
    "        # grammar_2\n",
    "        return grammar_2(cursor, tokens)\n",
    "\n",
    "\n",
    "def grammar_2(cursor, tokens):\n",
    "    \"\"\"\n",
    "    Grammar for 'grammar_2:= (grammar_1) or grammar_1 | (grammar_1) and grammar_1 | (grammar_1) | interactor'.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(tokens) <= 0:\n",
    "        raise ValueError(\"Invalid Syntax\")\n",
    "\n",
    "    elif tokens[0] == \"(\":\n",
    "        # (grammar_1) or grammar_1 | (grammar_1) and grammar_1| (grammar_1)\n",
    "\n",
    "        # token after the last occurring ')'\n",
    "        token_after_last_closer = helper.remove_parentheses(tokens)\n",
    "\n",
    "        if token_after_last_closer == 'or':\n",
    "            # split tokens by the first occurring 'or' and store the tokens\n",
    "            # before and after the 'or' in a dictionary\n",
    "\n",
    "            or_dictionary = helper.split_by(tokens, 'or')\n",
    "            return grammar_or(grammar_1(cursor, or_dictionary.get(0)), grammar_1(cursor, or_dictionary.get(1)))\n",
    "\n",
    "        elif token_after_last_closer == 'and':\n",
    "            # split tokens by the first occurring 'and' and store the tokens\n",
    "            # before and after the 'and' in a dictionary\n",
    "\n",
    "            and_dictionary = helper.split_by(tokens, 'and')\n",
    "            return grammar_and(grammar_1(and_dictionary.get(0)), grammar_1(and_dictionary.get(1)))\n",
    "\n",
    "        else:\n",
    "            # grammar_1; delegate to grammar_1\n",
    "            return grammar_1(cursor, tokens)\n",
    "\n",
    "    else:\n",
    "        # interactor; delegate to interactor\n",
    "        return interactor(cursor, tokens)\n",
    "\n",
    "\n",
    "def interactor(cursor, token):\n",
    "    \"\"\"\n",
    "    Grammar for 'interactor'.\n",
    "    \"\"\"\n",
    "\n",
    "    species = token[0]\n",
    "    return [[db_get_species_id_from_name(cursor, species)]]\n",
    "\n",
    "\n",
    "def grammar_output(tokens1, tokens2):\n",
    "    \"\"\"\n",
    "    Grammar for '='.\n",
    "    \"\"\"\n",
    "\n",
    "    grammar_output_dict = {}\n",
    "\n",
    "    for token1 in tokens1:\n",
    "        grammar_output_dict[tuple(token1)] = tuple(tokens2)\n",
    "\n",
    "    return grammar_output_dict\n",
    "\n",
    "\n",
    "def grammar_or(tokens1, tokens2):\n",
    "    \"\"\"\n",
    "    Grammar for 'or'.\n",
    "    \"\"\"\n",
    "\n",
    "    return tokens1 + tokens2\n",
    "\n",
    "\n",
    "def grammar_and(tokens1, tokens2):\n",
    "    \"\"\"\n",
    "    Grammar for 'and'.\n",
    "    \"\"\"\n",
    "\n",
    "    grammar_and_output = []\n",
    "\n",
    "    for token1 in tokens1:\n",
    "        for token2 in tokens2:\n",
    "            grammar_and_output.append(helper.uniquely_merge_list_of_lists([token1, token2]))\n",
    "\n",
    "    return grammar_and_output\n",
    "\n",
    "\n",
    "def parse_logic(cursor, logic_input):\n",
    "    \"\"\"\n",
    "    Parse a logic input into one or more atomized and equivalent logics.\n",
    "    \"\"\"\n",
    "\n",
    "    split_logic_input = logic_input.split()\n",
    "\n",
    "    # begins recursive logic parse\n",
    "    return grammar_0(cursor, split_logic_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_sbider_path_memory(input_dictionary, activated_paths, from_operon):\n",
    "    activated_ope_dic = {}\n",
    "    activated_spe_dic = {}\n",
    "    for path_idx, ope_spe_path in enumerate(activated_paths):\n",
    "        activated_ope_dic[path_idx] = ope_spe_path[0]\n",
    "        activated_spe_dic[path_idx] = ope_spe_path[1]\n",
    "    final_operon_requirement = input_dictionary[from_operon]\n",
    "    activating_ope_list = []\n",
    "    for path_idx, spe_produced in activated_spe_dic.items():\n",
    "        for a_spe_produced in spe_produced:\n",
    "            for and_spe_required in final_operon_requirement:\n",
    "                if a_spe_produced in and_spe_required:\n",
    "                    activating_ope_list.extend(activated_ope_dic.get(path_idx))\n",
    "    return activating_ope_list\n",
    "\n",
    "\n",
    "def build_sbider_path_memory_tree(input_dictionary, activated_paths, start_operon):\n",
    "    root_ope = node.Node(start_operon)\n",
    "    temp_queue_ope = [root_ope]\n",
    "    temp_memory = []\n",
    "    while len(activated_paths) > 0 and len(temp_queue_ope) > 0:\n",
    "        from_node = temp_queue_ope.pop(0)\n",
    "        from_operon = from_node.value\n",
    "        children_operon = search_sbider_path_memory(input_dictionary, activated_paths, from_operon)\n",
    "        if len(children_operon) > 0:\n",
    "            for child_operon in children_operon:\n",
    "                if child_operon not in temp_memory:\n",
    "                    child_node = node.Node(child_operon)\n",
    "                    from_node.append_child(child_node)\n",
    "                    temp_queue_ope.append(child_node)\n",
    "                    temp_memory.append(child_operon)\n",
    "    return root_ope.get_path_from_all_leaf()\n",
    "\n",
    "\n",
    "def build_indirect_sbider_path(input_dictionary,\n",
    "                               repressor_dictionary,\n",
    "                               output_dictionary,\n",
    "                               input_species_list,\n",
    "                               output_species_list,\n",
    "                               path_queue,\n",
    "                               final_operon_path_list,\n",
    "                               memory_operon,\n",
    "                               memory_species,\n",
    "                               activated_paths):\n",
    "    temp_memory_species = []\n",
    "    for an_operon in set(input_dictionary.keys()) - set(memory_operon):\n",
    "\n",
    "        if helper.promoter_activation(input_dictionary, repressor_dictionary, an_operon, [], memory_species, True):\n",
    "\n",
    "            just_produced_species = output_dictionary[an_operon]\n",
    "            just_produced_unique_species = helper.uniquely_merge_multi_dimensional_list_of_lists(just_produced_species)\n",
    "\n",
    "            if helper.match_any_list(just_produced_species, output_species_list):\n",
    "\n",
    "                if len(activated_paths) > 1:\n",
    "                    ope_path_backward = build_sbider_path_memory_tree(input_dictionary,\n",
    "                                                                      activated_paths,\n",
    "                                                                      an_operon)\n",
    "                    final_operon_path_list.extend(ope_path_backward)\n",
    "            else:\n",
    "                if an_operon not in memory_operon:\n",
    "                    path_queue.append(([an_operon], just_produced_unique_species))\n",
    "                    memory_operon.append(an_operon)\n",
    "                    memory_operon = helper.remove_duplicates_within_list(memory_operon)\n",
    "                    temp_memory_species.extend(just_produced_unique_species)\n",
    "                    activated_paths.append([[an_operon], just_produced_unique_species])\n",
    "\n",
    "    memory_species.extend(temp_memory_species)\n",
    "    memory_species = helper.remove_duplicates_within_list(memory_species)\n",
    "\n",
    "    if len(path_queue) > 0:\n",
    "        build_direct_sbider_path(input_dictionary,\n",
    "                                 repressor_dictionary,\n",
    "                                 output_dictionary,\n",
    "                                 input_species_list,\n",
    "                                 output_species_list,\n",
    "                                 path_queue,\n",
    "                                 final_operon_path_list,\n",
    "                                 memory_operon,\n",
    "                                 memory_species,\n",
    "                                 activated_paths,\n",
    "                                 True)\n",
    "\n",
    "\n",
    "def build_direct_sbider_path(input_dictionary,\n",
    "                             repressor_dictionary,\n",
    "                             output_dictionary,\n",
    "                             input_species_list,\n",
    "                             output_species_list,\n",
    "                             path_queue,\n",
    "                             final_operon_path_list,\n",
    "                             memory_operon,\n",
    "                             memory_species,\n",
    "                             activated_paths,\n",
    "                             indirect_flag):\n",
    "    while len(path_queue) != 0:\n",
    "\n",
    "        (previously_visited_operon_list, just_previously_produced_species_list) = path_queue.pop(0)\n",
    "\n",
    "        for an_operon in set(input_dictionary.keys()) - set(\n",
    "                helper.uniquely_merge_multi_dimensional_list_of_lists(previously_visited_operon_list)):\n",
    "            if an_operon not in memory_operon:\n",
    "\n",
    "\n",
    "                if helper.promoter_activation(input_dictionary, repressor_dictionary, an_operon,\n",
    "                                              just_previously_produced_species_list, memory_species, False):\n",
    "\n",
    "                    visited_operon_list = previously_visited_operon_list + [an_operon]\n",
    "                    just_produced_species = output_dictionary[an_operon]\n",
    "                    just_produced_unique_species = helper.uniquely_merge_multi_dimensional_list_of_lists(\n",
    "                        just_produced_species)\n",
    "\n",
    "                    if helper.match_any_list(just_produced_species, output_species_list):\n",
    "\n",
    "                        if not indirect_flag:\n",
    "                            final_operon_path_list.append(visited_operon_list)\n",
    "                    else:\n",
    "                        path_queue.append((visited_operon_list, just_produced_unique_species))\n",
    "                        memory_operon.append(an_operon)\n",
    "                        memory_operon = helper.remove_duplicates_within_list(memory_operon)\n",
    "                        memory_species.extend(just_produced_unique_species)\n",
    "                        memory_species = helper.remove_duplicates_within_list(memory_species)\n",
    "\n",
    "                    activated_paths.append([[an_operon], just_produced_unique_species])\n",
    "\n",
    "    if indirect_flag:\n",
    "        build_indirect_sbider_path(input_dictionary,\n",
    "                                   repressor_dictionary,\n",
    "                                   output_dictionary,\n",
    "                                   input_species_list,\n",
    "                                   output_species_list,\n",
    "                                   path_queue,\n",
    "                                   final_operon_path_list,\n",
    "                                   memory_operon,\n",
    "                                   memory_species,\n",
    "                                   activated_paths)\n",
    "    return final_operon_path_list\n",
    "\n",
    "\n",
    "def get_sbider_path(inp_dic,\n",
    "                    rep_dic,\n",
    "                    outp_dic,\n",
    "                    inp_spe,\n",
    "                    outp_spe,\n",
    "                    indirect_flag=False):\n",
    "    final_ope_path = []\n",
    "    path_queue = [([], inp_spe)]\n",
    "    memory_ope = []\n",
    "    memory_spe = []\n",
    "    memory_spe.extend(inp_spe)\n",
    "    activated_paths = []\n",
    "    build_direct_sbider_path(inp_dic,\n",
    "                             rep_dic,\n",
    "                             outp_dic,\n",
    "                             inp_spe,\n",
    "                             outp_spe,\n",
    "                             path_queue,\n",
    "                             final_ope_path,\n",
    "                             memory_ope,\n",
    "                             memory_spe,\n",
    "                             activated_paths,\n",
    "                             indirect_flag)\n",
    "    if len(final_ope_path) > 0:\n",
    "        final_ope_path = helper.remove_duplicated_lists_within_a_list_of_lists(final_ope_path)\n",
    "\n",
    "    return final_ope_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_sbider_network(dir_database, user_query, indirect=False):\n",
    "\n",
    "    # Access database\n",
    "    f_database = dir_database + \"/SBiDer.db\"\n",
    "    conn, cur = db_open(f_database)\n",
    "    \n",
    "    # Print all tables\n",
    "    #db_print_all_tables(cur)\n",
    "\n",
    "    # Dictionary of fragmented user inputs that satisfy user query\n",
    "    logic_dictionary = parse_logic(cur, user_query)\n",
    "\n",
    "    # Dictionaries of: Operon <-> InputSpecies & Operon <-> OutputSpecies\n",
    "    input_dictionary, output_dictionary = make_ope_id_spe_id_dics(cur)\n",
    "\n",
    "    # Dictionary of: Operon <-> Repressor\n",
    "    repressor_dictionary = make_ope_id_rep_spe_id_dic(cur)\n",
    "\n",
    "    # Build operon path for each fragmented user input, which satisfies user query\n",
    "    all_operon_path = []\n",
    "    for input_species, output_species_list in logic_dictionary.items():\n",
    "\n",
    "        operon_path_per_start_species = [input_species]\n",
    "        for output_species in output_species_list:\n",
    "            operon_path_list = get_sbider_path(input_dictionary,\n",
    "                                                        repressor_dictionary,\n",
    "                                                        output_dictionary,\n",
    "                                                        list(input_species),\n",
    "                                                        output_species,\n",
    "                                                        indirect)\n",
    "\n",
    "            operon_path_per_start_species.extend(operon_path_list)\n",
    "        all_operon_path.append(operon_path_per_start_species)\n",
    "        \n",
    "        return operon_path_per_start_species\n",
    "\n",
    "        # Create JSON file needed to display the found genetic circuit\n",
    "        #path_json = grapher.create_subnetwork_json_string(cur, operon_path_per_start_species, database_file)\n",
    "\n",
    "        #return path_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "##Arguments:\n",
    "\n",
    "# 0=sbider_network_builder.py\n",
    "# 1=database path \n",
    "# 2=user input\n",
    "# 3=indirect flag\n",
    "\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # path to the directory that contains the SBiDer database\n",
    "    dir_database = sys.argv[1]\n",
    "\n",
    "    # boolean flag for indirect path\n",
    "    last_argv = str(sys.argv[-1]).lower()\n",
    "\n",
    "    if last_argv == 't':\n",
    "        user_input = \" \".join(sys.argv[2:-1:])\n",
    "        indirect_flag = True\n",
    "\n",
    "    elif last_argv == 'f':\n",
    "        user_input = \" \".join(sys.argv[2:-1:])\n",
    "        indirect_flag = False\n",
    "\n",
    "    else:\n",
    "        user_input = \" \".join(sys.argv[2::])\n",
    "        indirect_flag = False\n",
    "\n",
    "    print(\"*dir_database \" + dir_database)\n",
    "    print(\"*user input \" + user_input)\n",
    "    print(\"*indirect flag \"+ str(indirect_flag) +\"\\n\")\n",
    "\n",
    "    # search SBiDer network\n",
    "    final_path_json = build_sbider_network(dir_database, user_input, indirect_flag)\n",
    "    \n",
    "    # print out the final JSON path\n",
    "    print(final_path_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laci and iptg = gfp\n",
      "['1-1 pLuxI_pLacO--->gfp', '15-1 pLac--->gfp', '18-1 pLac--->gfp', '46-2 pLac--->gfp', '50-1 pLac_pBAD--->gfp', '57-1 pLac_pBAD--->LuxR_luxI']\n",
      "\n",
      "ohhl and luxr = gfp\n",
      "['1-1 pLuxI_pLacO--->gfp']\n",
      "\n",
      "arac and lara = yfp\n",
      "['2-1 pBAD--->yfp', '5-1 pBAD_pTet--->yfp', '6-1 pBAD_pLasI--->yfp']\n",
      "\n",
      "tetr and atc = yfp\n",
      "['3-1 pTet--->yfp', '5-1 pBAD_pTet--->yfp', '7-1 pTet_pLasI--->yfp', '71-2 pTet--->yfp']\n",
      "\n",
      "lasr and pai-1 = yfp\n",
      "['4-1 pLasI--->yfp', '6-1 pBAD_pLasI--->yfp', '7-1 pTet_pLasI--->yfp']\n",
      "\n",
      "arac and lara = yfp\n",
      "['2-1 pBAD--->yfp', '5-1 pBAD_pTet--->yfp', '6-1 pBAD_pLasI--->yfp']\n",
      "\n",
      "tetr and atc = ci\n",
      "['10-1 pTet_pLas--->cI', '8-1 pBAD_pTet--->cI']\n",
      "\n",
      "arac and lara = ci\n",
      "['57-1 pLac_pBAD--->LuxR_luxI', '79-1 pBAD--->cI', '8-1 pBAD_pTet--->cI', '9-1 pBAD_pLas--->cI']\n",
      "\n",
      "lasr and pai-1 = ci\n",
      "['10-1 pTet_pLas--->cI', '9-1 pBAD_pLas--->cI']\n",
      "\n",
      "rhlr and pai-2 = ci\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-42042efec219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtests\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-42042efec219>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(inpt)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Build path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0moperon_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_sbider_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_database\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Get operon path with names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-dfdd52ff94bf>\u001b[0m in \u001b[0;36mbuild_sbider_network\u001b[0;34m(dir_database, user_query, indirect)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Dictionary of fragmented user inputs that satisfy user query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mlogic_dictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_logic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Dictionaries of: Operon <-> InputSpecies & Operon <-> OutputSpecies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-bdc9e91f87ee>\u001b[0m in \u001b[0;36mparse_logic\u001b[0;34m(cursor, logic_input)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;31m# begins recursive logic parse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrammar_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_logic_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-bdc9e91f87ee>\u001b[0m in \u001b[0;36mgrammar_0\u001b[0;34m(cursor, tokens)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0minput_output_dictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_by\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     return grammar_output(grammar_1(cursor, input_output_dictionary[0]), \n\u001b[0m\u001b[1;32m     13\u001b[0m                           grammar_1(cursor, input_output_dictionary[1]))\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-bdc9e91f87ee>\u001b[0m in \u001b[0;36mgrammar_1\u001b[0;34m(cursor, tokens)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# before and after the 'and' in a dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mand_dictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_by\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'and'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgrammar_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrammar_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mand_dictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrammar_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mand_dictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-bdc9e91f87ee>\u001b[0m in \u001b[0;36mgrammar_2\u001b[0;34m(cursor, tokens)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# interactor; delegate to interactor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minteractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-bdc9e91f87ee>\u001b[0m in \u001b[0;36minteractor\u001b[0;34m(cursor, token)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mspecies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdb_get_species_id_from_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-cf84e820d34c>\u001b[0m in \u001b[0;36mdb_get_species_id_from_name\u001b[0;34m(cursor, species_name)\u001b[0m\n\u001b[1;32m     83\u001b[0m                       \u001b[0;34m[\u001b[0m\u001b[0;34m\"'%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mspecies_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                       \"\")\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0ma_cur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "def test(inpt):\n",
    "\n",
    "    # Input\n",
    "    user_input = inpt\n",
    "    \n",
    "    # Accesss database\n",
    "    dir_database = '/Users/Kwat/BInf/SBiDer/CircuitNetwork/web'\n",
    "    f_database = dir_database + \"/SBiDer.db\"\n",
    "    conn, cur = db_open(f_database)\n",
    "\n",
    "    # Build path\n",
    "    operon_path = build_sbider_network(dir_database, user_input, False)\n",
    "\n",
    "    # Get operon path with names\n",
    "    lst_opeNames = []\n",
    "    for operon in operon_path[1:]:\n",
    "        i = operon[0]\n",
    "        name = i + \" \" + db_get_operon_name_from_id(cur, str(operon[0]))\n",
    "        lst_opeNames.append(name)\n",
    "    print(lst_opeNames)\n",
    "    print()\n",
    "\n",
    "# Make tester\n",
    "test_file = open('/Users/Kwat/BInf/SBiDer/CircuitNetwork/web/sbider_db_all_io.txt')\n",
    "tests = []\n",
    "for l in test_file:\n",
    "    tests.append(l[0:-1])\n",
    "tests[-1] = tests[-1]+'db'\n",
    "\n",
    "# Test\n",
    "for t in tests:\n",
    "    print(t)\n",
    "    test(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
